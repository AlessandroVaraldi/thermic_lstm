# LSTM × Therminic

Tiny but complete reference that joins a **single-RC thermal network** and a lightweight **dot-product-attention LSTM**.  
The aim is real-time prediction of MOSFET junction temperature, with a clean comparison against the analytical ODE.

---

## 1. Project goals

* Show a practical recipe for mixing first-principle physics with deep learning in fewer than 500 lines of code.  
* Keep every step reproducible and thoroughly commented so newcomers can follow along.  
* Provide an Optuna pipeline that tunes both the pure data-driven model and the physics-informed model under the same budget.

---

## 2. Directory map

    lstm-therminic/
    ├── data_sets/        ← raw and augmented CSV files
    ├── plots/            ← figures generated by the scripts
    ├── src/
    │   ├── config.py     ← global constants and paths
    │   ├── data_utils.py ← CSV I/O, power calculation, MC-dropout, augmentation
    │   ├── dataset.py    ← sliding-window PyTorch Dataset
    │   ├── models.py     ← single-layer LSTM with attention
    │   ├── phys_models.py← thermal ODE, steady-state and residual helpers
    │   └── train.py      ← training loop with optional physics terms and AMP
    └── optuna_run.py     ← two-phase hyper-parameter search and evaluation

---

## 3. System requirements

| Component | Minimum version | Notes |
|-----------|-----------------|-------|
| Python    | 3.9             | tested on CPython 3.12 |
| CUDA SDK  | 11.8            | required only for GPU training |
| GPU       | 4 GB VRAM, compute ≥ 7.0 | falls back to CPU if absent |
| PyTorch   | 2.2 (built for your CUDA) | verify with `python -m torch.utils.collect_env` |
| Optuna    | 3.6             | hyper-parameter optimisation |
| SciPy / NumPy / Matplotlib | latest | plotting and ODE reference |
| Requests  | any             | Telegram notifications |
| Thop (optional) | any | FLOP and parameter counting |

Training is much faster on a mid-range GPU. Automatic mixed precision halves VRAM usage and speeds up training.

---

## 4. Installation

### 4.1 Create a virtual environment

    python -m venv .venv
    source .venv/bin/activate        # Linux / macOS
    REM .venv\Scripts\activate       # Windows PowerShell

### 4.2 Install Python packages

*GPU build (recommended)*

    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
    pip install -r requirements.txt

*CPU-only build*

    pip install torch torchvision torchaudio
    pip install -r requirements.txt

Optional extras

    pip install thop         # FLOP / parameter counter
    pip install requests     # already in requirements, used by Telegram helper

### 4.3 Check CUDA availability

    python - <<'PY'
    import torch, platform
    print("PyTorch", torch.__version__)
    print("CUDA available:", torch.cuda.is_available())
    if torch.cuda.is_available():
        print("GPU:", torch.cuda.get_device_name(0))
    PY

If CUDA is not available, reinstall the correct PyTorch wheel or update the NVIDIA driver.

---

## 5. Telegram bot notifications (optional)

1. Talk to **@BotFather**, create a new bot, and copy its token.  
2. Open a chat with the bot and send a message.  
3. Visit `https://api.telegram.org/bot<TOKEN>/getUpdates` and note the `chat.id`.  
4. Export two environment variables:

        export TELEGRAM_BOT_TOKEN="123456:ABCDEF..."
        export TELEGRAM_CHAT_ID="-987654321"

Notifications activate automatically when both variables are present. Omit them to disable the feature.

---

## 6. Quick start

    git clone https://github.com/your-org/lstm-therminic.git
    cd lstm-therminic

Run thirty trials per Optuna study (≈ 10 min on an RTX 3060):

    python optuna_run.py --trials 30 --device cuda --amp

Figures go to `plots/`, logs to `logs/`.

---

## 7. Configuration

Open `src/config.py` and tweak the essentials:

```
python
CSV_FILE    = "data_sets/my_record.csv"
WINDOW_SIZE = 128        # samples per window
LAMBDA_PHYS = 1e-2       # weight for physics loss
DEVICE      = "cuda"     # or "cpu"
````

All other modules import values from this file.

---

## 8. Training pipeline

1. **Architecture and learning-rate search** with `LAMBDA_PHYS = 0`.
2. **Physics-weight search** on the best architecture.
3. **Retrain** the top data-driven and physics-informed models on the full train + val split.
4. **Evaluate** both models and the one-RC ODE on the held-out test set.

Everything is automated in `optuna_run.py`.

---

## 9. Physics-informed loss

When `LAMBDA_PHYS > 0` two extra terms are added:

* Steady-state mismatch with the analytical solution `T_steady_state`.
* Mean-squared residual of the first-order energy balance over one time step.

Set `LAMBDA_PHYS = 0` to disable them.

---

## 10. License and citation

This project is released under the MIT License.

If it helps your research, please cite the accompanying Therminic 2025 paper:

```
@inproceedings{lstm_therminic_2025,
  author    = {Your Name and others},
  title     = {Physics-Informed LSTM for Real-Time Junction Temperature},
  booktitle = {Proc. Therminic},
  year      = {2025}
}
```
