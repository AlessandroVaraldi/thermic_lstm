// src/engine.h
#pragma once
#include <stdint.h>
#include "include/model_offsets.h"   // generated by exporter (MODEL_* + NORM_* + FC_*)

#ifdef __cplusplus
extern "C" {
#endif

#ifndef MODEL_LAYERS
# error "MODEL_LAYERS not defined. Include generated model_offsets.h."
#endif
#ifndef MODEL_HIDDEN
# error "MODEL_HIDDEN not defined. Include generated model_offsets.h."
#endif

// ---- convenience ----
#define QAT_LAYERS   (MODEL_LAYERS)
#define QAT_HIDDEN   (MODEL_HIDDEN)
#define QAT_INPUTS   (MODEL_INPUT)
#define QAT_STATE_LEN (QAT_LAYERS * QAT_HIDDEN)  // length of h[] and c[] in Q15

// ---- requant descriptor (acc -> Q-format) ----
typedef struct {
    int16_t mult_q15;  // Q15 multiplier
    int8_t  rshift;    // arithmetic right shift (>=0)
} rq_q15_t;

typedef struct {
    rq_q15_t ih;       // input-hidden path requant
    rq_q15_t hh;       // hidden-hidden path requant
} rq_pair_t;

// ---- one LSTM layer view into the packed blob ----
typedef struct {
    // weights view (packed, little-endian)
    const int8_t  *W_ih;   // [4H, IN], int8
    const int32_t *B_ih;   // [4H] or NULL, int32
    const int8_t  *W_hh;   // [4H, H],  int8
    const int32_t *B_hh;   // [4H] or NULL, int32

    // dims
    int16_t in;            // input features to this layer
    int16_t H;             // hidden size for this layer

    // activation scalers (exported per-layer)
    int16_t S_gate_q8;     // for sigmoid_q8
    int16_t S_tanhc_q8;    // for tanh_q8 on candidate/cell

    // per-gate requants to Q0.8 pre-activations
    // index: 0=i, 1=f, 2=g, 3=o
    rq_pair_t rq[4];
} lstm_layer_t;

// ---- FC head over [h_L | tanh(c_L)] ----
typedef struct {
    const int8_t  *W;      // [1, 2H], int8
    const int32_t *B;      // [1] or NULL, int32
    int16_t in;            // 2H
    // requant to Q15 output (y_norm in Q15)
    int16_t rqm;           // mult_q15
    int8_t  rqs;           // rshift
} fc_head_t;

// ---- model handle (stack/static) ----
typedef struct {
    int16_t L;                         // layers (=MODEL_LAYERS)
    int16_t H;                         // hidden (=MODEL_HIDDEN)
    lstm_layer_t layers[QAT_LAYERS];   // layer views bound to the blob
    fc_head_t    fc;                   // FC head view
} qat_model_t;

// ================= API =================

// Bind views (weights/bias/req) from packed blob into m->layers/m->fc.
// Pre: m allocated (stack or static). Blob is model_int8.bin already loaded in RAM.
void qat_bind_weights(qat_model_t *m, const uint8_t *blob);

// Zero hidden/cell states (size = QAT_STATE_LEN).
void lstm_reset(const qat_model_t *m, int16_t *h, int16_t *c);

// One step with *normalized* inputs (Q15).
// x_norm_q15: length MODEL_INPUT (e.g., 2). h,c: length QAT_STATE_LEN.
// Writes y_norm_q15 (Q15).
void lstm_step_normed(const qat_model_t *m,
                      const int16_t *x_norm_q15,
                      int16_t *h, int16_t *c,
                      int16_t *y_norm_q15);

// One step with *raw* inputs (Q15). Normalization is applied inside using NORM_AX*/BX*.
void lstm_step_raw(const qat_model_t *m,
                   const int16_t *x_raw_q15,
                   int16_t *h, int16_t *c,
                   int16_t *y_norm_q15);

#ifdef __cplusplus
}
#endif
