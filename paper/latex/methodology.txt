\section{Methodology}\label{sec:methodology}

\subsection{Problem Formulation}\label{ssec:formulation}
Let $P_t$ (W) denote instantaneous power loss and $T_{\mathrm{bp},t}$ (°C) the base-plate temperature.  
Given the most recent $L=128$ samples,
\[
\mathbf x_t=\bigl[P_{t-127},\dots,P_t,\,
                  T_{\mathrm{bp},t-127},\dots,T_{\mathrm{bp},t}\bigr]^\top
              \!\in\mathbb R^{256},
\]
the task is to predict the current junction temperature
\begin{equation}
\hat T_{\mathrm j,t}=f_{\boldsymbol\theta}\!\bigl(\mathbf x_t\bigr),
\label{eq:seq2scal}
\end{equation}
where $f_{\boldsymbol\theta}$ denotes a neural mapping with parameters $\boldsymbol\theta$.  
The window slides forward by one step at each instant, so~\eqref{eq:seq2scal} is evaluated without look-ahead.

% -------------------------------------------------------------------------
\subsection{Thermal Model Parameters Identification}\label{ssec:parameters}
The reduced single–RC lattice that underlies~\eqref{eq:tss}–\eqref{eq:ode} is governed by two lumped resistances—%
a {\em conduction} path \(R_{\theta,\mathrm{C}}\) that links the chip stack to the coolant, and an (almost open) {\em convective} path \(R_{\theta,\mathrm{V}}\) toward the ambient—as well as an effective thermal capacitance \(C_\theta\).  
All three constants are determined once, off-line, through a simple power-step experiment on the target module, following the procedure already adopted for ECU end-of-line characterization:

\begin{itemize}
  \item \textbf{Steady state.}  
        The asymptotic temperature rise \(\Delta T_\infty\) measured at several power levels gives the total resistance \(R_{\theta,\mathrm{tot}}=\Delta T_\infty/P\).  
        Subtracting the coolant-side contribution (obtained from an NTC sensor on the ceramic) isolates the chip-to-coolant branch \(R_{\theta,\mathrm{C}}\).
  \item \textbf{Transient.}  
        Fitting the initial ramp to the first-order response \(T(t)=T_\infty\!\bigl(1-e^{-t/\tau}\bigr)\) yields the time constant \(\tau\).  
        With \(R_{\theta,\mathrm{tot}}\) already known, the capacitance follows from \(C_\theta=\tau/R_{\theta,\mathrm{tot}}\).
\end{itemize}

Applying this workflow to the present SiC traction module delivers
\begin{equation}\label{eq:ident_params}
\begin{aligned}
R_{\theta,\mathrm{C}} &\approx 1.7\;\text{K\,W}^{-1},\\
R_{\theta,\mathrm{V}} &\approx 1.0\times10^{5}\;\text{K\,W}^{-1},\\
C_\theta &\approx 1.5\;\text{J\,K}^{-1}.
\end{aligned}
\end{equation}

The very large \(R_{\theta,\mathrm{V}}\) confirms that, over the time-scales of interest, almost all heat is removed by liquid cooling rather than air convection.  
Because these parameters change only with severe ageing or packaging alterations, they are frozen during training and deployment, allowing the neural network to focus exclusively on learning the fast nonlinear mapping in~\eqref{eq:seq2scal}.

% -------------------------------------------------------------------------
\subsection{Dataset Gathering}\label{ssec:hpo}

% -------------------------------------------------------------------------
\subsection{Attention-augmented LSTM Architecture}\label{ssec:architecture}
\paragraph{Network backbone.}
A single-layer LSTM with hidden width $H = 16$ processes each \((L, F{=}2)\) window, producing hidden states $\mathbf h_{1{:}L}$.  
Following~\cite{Bahdanau2016}, a scaled dot-product attention head computes
\[
\alpha_\tau = 
\frac{\exp(\mathbf h_\tau^{\!\top}\mathbf h_L)}
     {\sum_{\sigma=1}^L\exp(\mathbf h_\sigma^{\!\top}\mathbf h_L)},\qquad
\mathbf z_L = \sum_{\tau=1}^L \alpha_\tau \mathbf h_\tau,
\]
and predicts
\[
\hat T_{\mathrm j,t} = 
\mathbf w^{\!\top}\!\bigl[\mathbf h_L;\mathbf z_L\bigr] + b,
\]
with variational dropout ($p = 0.1$) before the final affine layer.  
The observer counts only \(\approx 1.3\) k parameters, ensuring micro-controller deployability.

\paragraph{Three-stage Bayesian hyper-parameter search.}
We performed three successive Optuna studies (30 trials each, median pruning):

\begin{enumerate}
  \item \emph{Architecture \& learning-rate search.}
        Physics penalties disabled ($\lambda_{\mathrm{ss}}=\lambda_{\mathrm{tr}}=0$), exploring  
        \(\textit{hidden\_size}\!\in\!\{8,16,24,32\}\),  
        \(\textit{num\_layers}\!\in\!\{1,2,3\}\) and  
        \(\text{lr}\!\in\![10^{-4},10^{-2}]\).
  \item \emph{Steady-state penalty search.}
        Best backbone frozen; $\lambda_{\mathrm{ss}}\!\in\![10^{-7},10^{-5}]$, $\lambda_{\mathrm{tr}}=0$.
  \item \emph{Transient penalty search.}
        $\lambda_{\mathrm{ss}}$ fixed; $\lambda_{\mathrm{tr}}\!\in\![10^{-7},10^{-5}]$.
\end{enumerate}

The search converged to the following \emph{optimal} hyper-parameters:
\[
\begin{aligned}
&(H,\textit{layers},\text{lr})      =(16,\,1,\,2.5\times10^{-3}),\\
&(\lambda_{\mathrm{ss}},\lambda_{\mathrm{tr}})=(9.78\times10^{-6},\,2.15\times10^{-6}).
\end{aligned}
\]

% -------------------------------------------------------------------------
\subsection{Physics-informed Loss}\label{ssec:loss}
To keep predictions thermodynamically plausible, we augment the mean-squared data term with two residuals derived from the single-RC ladder:

\begin{align}
r_{\mathrm{ss},t} &=
\underbrace{\hat T_{\mathrm j,t}}_{\text{network}}
-\underbrace{\bigl(T_{\mathrm{bp},t}+P_t R_{\theta,\mathrm C}\bigr)}_{\text{steady-state RC}},
\label{eq:r_ss_explicit}\\[4pt]
r_{\mathrm{tr},t} &=
\underbrace{%
  C_\theta\tfrac{\hat T_{\mathrm j,t}-\hat T_{\mathrm j,t-1}}{\Delta t}%
}_{\mathclap{\scriptsize\text{thermal inertia}}}
-\Bigl(
  \underbrace{P_t}_{\mathclap{\scriptsize\text{loss input}}}
  +\underbrace{\tfrac{T_{\mathrm{bp},t}-\hat T_{\mathrm j,t}}{R_{\theta,\mathrm C}}}_{\mathclap{\scriptsize\text{conduction}}}
  +\underbrace{\tfrac{T_{\mathrm{env}}-\hat T_{\mathrm j,t}}{R_{\theta,\mathrm V}}}_{\mathclap{\scriptsize\text{convection}}}
\Bigr).
\label{eq:r_tr_explicit}
\end{align}

\noindent
Minimising
\begin{equation}
\mathcal L =
\frac1N\sum_{t}
\bigl(\hat T_{\mathrm j,t}-T_{\mathrm j,t}^{\mathrm{ref}}\bigr)^2
+\lambda_{\mathrm{ss}}\,r_{\mathrm{ss},t}^2
+\lambda_{\mathrm{tr}}\,r_{\mathrm{tr},t}^2
\label{eq:loss_total}
\end{equation}
balances data fidelity with steady-state consistency ($r_{\mathrm{ss},t}$) and transient energy conservation ($r_{\mathrm{tr},t}$).

% -------------------------------------------------------------------------
\subsection{Uncertainty Evaluation}\label{ssec:uncertainty}
In a safety-critical automotive context, point forecasts are not enough: confidence intervals are needed for dynamic derating.  
Predictive reliability is assessed with \emph{Monte-Carlo dropout}.  
During \emph{training} we apply variational dropout ($p=0.1$) to both the LSTM output and the regression head.  
At \emph{test} time the same dropout layer stays active, so each input window is forwarded $S=30$ times, yielding an ensemble of stochastic predictions without any additional memory overhead.  
Let $\hat T_{\mathrm j,t}^{(s)}$ be the $s$-th prediction; the predictive mean and standard deviation are
\begin{align}
\mu_t    &= \frac1S\sum_{s=1}^S \hat T_{\mathrm j,t}^{(s)},\\[4pt]
\sigma_t &= \sqrt{\frac1S\sum_{s=1}^S\bigl(\hat T_{\mathrm j,t}^{(s)}-\mu_t\bigr)^2}.
\end{align}
We report the mean-squared error of $\mu_t$ and the empirical 95 \% coverage of $[\mu_t-1.96\,\sigma_t,\;\mu_t+1.96\,\sigma_t]$.

% -------------------------------------------------------------------------
\subsection{Training Protocol and Deployment}\label{ssec:train}
\emph{Implementation.}  
The pipeline is written in \texttt{PyTorch 2.5} and leverages automatic mixed-precision when a CUDA-capable device is available.  
Mini-batches of 32 windows are processed with Adam; gradients are clipped to $\lVert\nabla\rVert_2\le1$.  
Early stopping monitors the validation loss with a patience of five epochs and halts after at most one thousand epochs.

\emph{Hardware.}  
All experiments run in real time on a single mid-range GPU (e.g.\ laptop-grade RTX 3060).  
Inference on the trained model fits comfortably within the SRAM and compute budget of recent automotive MCUs, enabling on-board deployment without external accelerators.

\emph{Reproducibility.}  
A single command reproduces the full three-stage search; omitting the search flags defaults to the best hyper-parameters above, shortening training to a few minutes.  Random seeds are fixed throughout to guarantee identical splits and weight initialisations across runs.